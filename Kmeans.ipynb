{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Abstract  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is one of the most common data processing algorithms. The algorithms obtain an excellent final solution by employing proper initialization. However, from a theoretical perspective concerning quality and efficiency, k-means is not a suitable clustering algorithm. This is because in the worst case it produces a final solution which is locally optimal, although it doesn't meet the typical global optimum. Besides, this paper will discuss the keys that were applied in the process improving k-means from traditional to the current modern algorithm. The big step taken was to develop the initialization procedure of traditional k-means which changed the performance of Lloyd's iteration. The improvement resulted in k-means++ algorithm which improves upon the running time of iteration. The algorithm picks only the first cluster center consistently at random from the data and probability is used in selecting the subsequent cluster center. Initialization of k-means++ leads to an O(log K) which is an approximation of the solution optimum. However, K-means++ algorithm has its downside which is being intrinsically sequential. Although it has a total running of 0(nkd), its k-clustering is similar to that of a single Lloyd's iteration. The downside led to obtainment a parallel version of k-means++ known as k-means||. The k-means|| algorithm uses the idea of sampling O(k) points per round, and the process is repeated to O(log n) rounds. Moreover, at the end the algorithm only O(k log n) points are left, which form a solution in a constant factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data mining and processing, some specifics algorithms are applied to cluster or group the data to meaningful information. Those algorithms are referred as clustering algorithms. In this case, scalable k-means paper is used in the research about of k-means algorithms, their efficiency in operation and the solution they offer in data mining processes. K-means clustering algorithms are used when data is unbalanced that is data without defined groups or categories (Bahmani, Moseley, Vattani, Kumar & Vassilvitskii, 2012). These algorithms aim to group data into clusters whereby variable k represents the number of groups or cluster. Besides, the algorithm works iteratively to allocate every data point to a single k group by use of provided attributes. It should be noted that data points are clustered on the ground of feature similarity. K-means results in centroids of the k clusters that are used in new data labeling and markers for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Application of the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means algorithm can be useful in almost all the fields. The algorithm is used in data clustering applications in different areas (Bahmani, Moseley, Vattani, Kumar & Vassilvitskii, 2012). For instance, clustering helps marketers advance their customers base as well as work on the goal areas. The k-means help group people using different criteria including purchasing power and willingness. The grouping is on the basis of their similarity in several ways which are related to the product under deliberation. In addition, clustering can be used to study earthquake. K-means analyses shows the next likely location where an earthquake can transpire by use of available data on the areas hit by the quake in a region. Moreover, k-means algorithm is mostly applied in large database management systems whereby data is mined, cleaned and analyzed for useful information. The advantage of k-means is that it is computationally faster than other clustering algorithms such as hierarchical clustering algorithm. The main disadvantage is that k-means cannot grip non-globular data of diverse sizes and densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance as dist\n",
    "import collections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize cluster centroids $\\mu_1, \\mu_2....\\mu_k \\in \\mathbb{R}^n$ randomly** <br>\n",
    "Repeat until convergence:{<br> \n",
    "+ For every i, set <br>\n",
    "> $c^{(i)}:= argmin||x^{(i)} - u_j||^2$  <br>\n",
    "+ For every j, set<br>\n",
    "> $\\mu_j := \\frac{\\sum_{i=1}^{m} 1 \\{c^{(i) = j}\\}x^{(i)}}{\\sum_{i=1}^{m} 1\\{c^{(i) = j}\\}}$<br>\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k, centroids, max_iter=10000):\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        cdist = (dist.cdist(data, centroids))**2\n",
    "        labels = np.argmin(cdist, axis=1)\n",
    "        update_centroids = np.zeros(centroids.shape)\n",
    "        for j in range(k):\n",
    "            # check if the centroid is the closest to some data point\n",
    "            if sum(labels == j) != 0:\n",
    "                update_centroids[j] = np.mean(data[labels ==j], axis=0)\n",
    "            else:\n",
    "                # if not, leave the lone centroid unmoved\n",
    "                update_centroids[j] = centroids[j]\n",
    "                \n",
    "        if np.allclose(update_centroids, centroids):\n",
    "            print(\"Algorithm converged after\", i, \"iterations.\")\n",
    "            return centroids\n",
    "        else:\n",
    "            centroids = update_centroids\n",
    "        \n",
    "    print(\"Warning: maximum number of iterations reached. Failed to converge.\")\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random initial centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm converged after 21 iterations.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    k, n, d = 20, 1000, 15\n",
    "    \n",
    "    mean, cov = np.zeros(d), np.eye(d)\n",
    "    data = np.random.multivariate_normal(mean, cov, n)\n",
    "    #random initial \n",
    "    initial_centers = np.random.multivariate_normal(mean, cov, k)\n",
    "    \n",
    "    centroids = kmeans(data, k, initial_centers)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Kmeans++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose the centers one by onte in a controlled fasion, where the current set of chosen centers will stochastically bias the choice of the next center**<br>\n",
    "\n",
    "First sample a point uniformly at random from X : C<br>\n",
    "Set how many clusters are needed: k <br>\n",
    "Distance between points and centers: $d^2(x, C)$<br>\n",
    "Compute the cost of X: $\\phi x(C) = \\sum_{x\\in X} min_{i=1,..k} ||y - c_i||^2$<br>\n",
    "While |C| < k do:\n",
    "+ Sample x $\\in$ X with probability $p_x = \\frac{d^2(x, C)}{\\phi x(C)}$ <br>\n",
    "+ Stack C with {x}: C = C $\\cup$ {x} <br> \n",
    "\n",
    "end while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pp(weights, data, k):\n",
    "    first_random = np.random.choice(data.shape[0], 1)\n",
    "    C = data[first_random, :]\n",
    "    \n",
    "    for i in range(k-1):\n",
    "        cdist = (dist.cdist(data, C))**2\n",
    "        cdist_min = np.min(cdist, axis = 1)* weights\n",
    "        prob = cdist_min/np.sum(cdist_min)\n",
    "        new_center = np.random.choice(data.shape[0],1, p=prob)\n",
    "        C = np.vstack([C, data[new_center,:]])\n",
    "        \n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal weights for kmenas++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm converged after 16 iterations.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    k, n, d = 20, 1000, 15\n",
    "    \n",
    "    mean, cov = np.zeros(d), np.eye(d)\n",
    "    data = np.random.multivariate_normal(mean, cov, n)\n",
    "    #equal weights for kmenas++\n",
    "    initial_centers = kmeans_pp(1, data, 20)\n",
    "    \n",
    "    centroids = kmeans(data, k, initial_centers)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Kmeans_II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel version of Kmeans++**<br>\n",
    "\n",
    "First set an oversampling factor l = $\\Omega(K)$<br>\n",
    "Sample a point uniformly at random from X : C<br>\n",
    "Set how many clusters are needed: k <br>\n",
    "Distance between points and centers: $d^2(x, C)$<br>\n",
    "Compute the cost of X: $\\phi x(C) = \\sum_{x\\in X} min_{i=1,..k} ||y - c_i||^2$<br>\n",
    "\n",
    "for O(log($\\phi$)) times do:\n",
    "+ Sample each point x $\\in$ X independently with probability $p_x = \\frac{l * d^2(x, C)}{\\phi x(C)}$ <br>\n",
    "+ Stack C with {x}: C = C $\\cup$ {x} <br> \n",
    "\n",
    "end for<br>\n",
    "For each point x$\\in$ C, compute $w_x$ to be the number of points in X closer than other points in C<br>\n",
    "Recluster the weighted points in C into k clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(C, data): \n",
    "    weights=np.zeros(C.shape[0])\n",
    "    cdist = (dist.cdist(data,C))**2\n",
    "    min_cdist = np.argmin(cdist, axis = 1)\n",
    "    count = collections.Counter(min_cdist) \n",
    "    weights = list(collections.OrderedDict(sorted(count.items(), key=lambda x: x[0])).values())\n",
    "    weights=np.array(weights)/sum(weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_II(data, k, l, max_iter=10000):\n",
    "    first_random = np.random.choice(data.shape[0], 1)\n",
    "    C = data[first_random, :]\n",
    "    \n",
    "    cdist = (dist.cdist(data, C))**2\n",
    "    cdist_min = np.min(cdist, axis = 1)\n",
    "    cost_phi = np.sum(cdist_min)\n",
    "    \n",
    "    for i in range(int(round(np.log(cost_phi)))):\n",
    "        cdist = (dist.cdist(data, C))**2\n",
    "        cdist_min = np.min(cdist, axis = 1)\n",
    "        prob = cdist_min * l/np.sum(cdist_min)\n",
    "        for j in range(data.shape[0]):\n",
    "            if np.random.uniform() <= prob[j] and data[j,:] not in C:\n",
    "                C = np.vstack([C, data[j,:]])\n",
    "   \n",
    "    weights= get_weight(C, data)\n",
    "\n",
    "    return kmeans_pp(weights, C,k)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial centroids with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm converged after 20 iterations.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    k, n, d = 20, 1000, 15\n",
    "    \n",
    "    mean, cov = np.zeros(d), np.eye(d)\n",
    "    data = np.random.multivariate_normal(mean, cov, n)\n",
    "    #initial with weight \n",
    "    initial_centers = kmeans_II(data, 20, 10)\n",
    "    \n",
    "    centroids = kmeans(data, k, initial_centers)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
